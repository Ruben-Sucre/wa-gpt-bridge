# wa-gpt-bridge

WhatsApp â†” OpenAI chatbot con orquestaciÃ³n n8n usando FastAPI, Redis y Docker.

> ðŸ“– **Para deployment en producciÃ³n, consulta la [GuÃ­a de Deployment](DEPLOYMENT.md)** con instrucciones detalladas de configuraciÃ³n, troubleshooting y monitoreo.

## Arquitectura

- **n8n**: Orquestador de workflows (puerto 5678)
- **bot**: Middleware FastAPI para procesamiento de mensajes (puerto 8000)
- **redis**: Almacenamiento de historial conversacional (puerto interno 6379)

## Flujo de Datos

```
Usuario WhatsApp â†’ Meta WhatsApp Cloud API â†’ ngrok
  â†’ FastAPI Bot (valida, limpia, consulta Redis, llama Gemini/OpenAI, responde)
  â†’ Meta WhatsApp Cloud API â†’ Usuario WhatsApp
```

## Levantar el stack

### Comando Ãºnico (recomendado)

```bash
./start.sh
```

Este script hace **en orden**:
1. `docker compose up -d` â€” levanta Redis y el Bot
2. `ngrok http 8000 --domain=...` â€” expone el bot con dominio fijo

> âš ï¸ **Siempre usa `./start.sh`** en lugar de levantar docker y ngrok por separado.
> ngrok debe apuntar al bot (`8000`), no a n8n (`5678`).

### URL pÃºblica fija (no cambia)

```
https://agonisingly-unapprehended-vernice.ngrok-free.dev/webhook/whatsapp
```

Esta URL ya estÃ¡ configurada en Meta. No necesitas cambiarla nunca.

### Token de WhatsApp (expira cada 24h en modo prueba)

Cuando el bot deje de responder, renueva el token en Meta â†’ WhatsApp â†’ API Setup, actualiza `.env`:

```bash
# Edita .env y actualiza WHATSAPP_TOKEN=...
docker compose up -d bot   # recarga el token sin bajar redis
```

## CaracterÃ­sticas

- âœ… System prompt cargado desde `services/bot/prompts/system_prompt.txt`
- âœ… Historial conversacional persistente en Redis (TTL 24h)
- âœ… **LÃ­mite de mensajes** - Solo Ãºltimos 20 mensajes para prevenir overflow de contexto
- âœ… **Rate limiting** - ProtecciÃ³n anti-spam (10 mensajes/minuto por usuario)
- âœ… ValidaciÃ³n de webhook de Meta (hub.challenge)
- âœ… Limpieza de texto y sanitizaciÃ³n
- âœ… AutenticaciÃ³n con header `x-bot-secret`
- âœ… **Health check mejorado** - Verifica Redis y credenciales de WhatsApp
- âœ… EnvÃ­o directo a WhatsApp Cloud API desde FastAPI
- âœ… Selector de LLM por variable de entorno (OpenAI o Gemini 2.0 Flash)

## Quick Start

### 1. Configurar variables de entorno

```bash
cp .env.example .env
# Edita .env y configura tus API keys
# LLM_PROVIDER=openai|gemini
# OPENAI_API_KEY=... (si usas OpenAI)
# OPENAI_MODEL=gpt-4o
# GOOGLE_API_KEY=... (si usas Gemini)
# GEMINI_MODEL=gemini-1.5-flash
```

### 2. Levantar servicios con Docker Compose

```bash
docker compose up --build -d
```

### 3. Verificar salud del bot

```bash
curl http://localhost:8000/health
```

### 4. Importar flujo n8n

1. Abre n8n en `http://localhost:5678` (usuario/contraseÃ±a en `.env`)
2. Import â†’ Selecciona `n8n/flows/wa-gpt-openai.json`
3. Configura credenciales de WhatsApp en n8n
4. Activa el workflow

### 5. Configurar webhook en Meta

1. Ve a Meta Business Console â†’ WhatsApp â†’ Configuration â†’ Webhooks
2. URL: `https://tu-dominio.com/webhook/whatsapp` (endpoint de n8n expuesto)
3. Verify Token: configura el mismo que uses en n8n
4. Suscribe a `messages`

## Estructura

```
wa-gpt-bridge/
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ .env.example                # Variables de entorno
â”œâ”€â”€ n8n/
â”‚   â””â”€â”€ flows/
â”‚       â””â”€â”€ wa-gpt-openai.json  # Workflow n8n
â””â”€â”€ services/
    â””â”€â”€ bot/
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ app/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ main.py           # FastAPI endpoints
        â”‚   â”œâ”€â”€ validation.py     # Modelos Pydantic
        â”‚   â”œâ”€â”€ cleaner.py        # SanitizaciÃ³n de texto
        â”‚   â”œâ”€â”€ memory.py         # Redis client
        â”‚   â”œâ”€â”€ openai_client.py  # Wrapper OpenAI
        â”‚   â””â”€â”€ whatsapp_client.py # Meta API client
        â””â”€â”€ prompts/
            â””â”€â”€ system_prompt.txt # PersonalizaciÃ³n del asistente
```

## API Endpoints

### `GET /health`
Health check del servicio con verificaciÃ³n de dependencias.

**Respuesta**:
```json
{
  "status": "ok",
  "llm_provider": "gemini",
  "checks": {
    "redis": "ok",
    "whatsapp_credentials": "ok"
  }
}
```

**Estados posibles**:
- `status`: `"ok"` (todo funcional) o `"degraded"` (Redis desconectado)
- `checks.redis`: `"ok"` o `"failed"`
- `checks.whatsapp_credentials`: `"ok"` o `"not_configured"`

### `POST /webhook/whatsapp`
Procesa mensajes de WhatsApp.

**Headers**:
- `x-bot-secret`: Token de autenticaciÃ³n (opcional, configurable en `.env`)

**Body**:
```json
{
  "from": "+1234567890",
  "text": "Hola, necesito ayuda"
}
```

**Respuesta**:
```json
{
  "delivered": true,
  "detail": null
}
```

## PersonalizaciÃ³n

### Modificar el System Prompt

Edita `services/bot/prompts/system_prompt.txt` para cambiar el comportamiento del asistente.

### Elegir proveedor LLM

- Define `LLM_PROVIDER=openai` o `LLM_PROVIDER=gemini` en `.env`.
- OpenAI usa `OPENAI_API_KEY` y `OPENAI_MODEL` (default: gpt-4o).
- Gemini usa `GOOGLE_API_KEY` y `GEMINI_MODEL` (default: gemini-1.5-flash).

### Ajustar parÃ¡metros de generaciÃ³n

En los clientes (`services/bot/app/openai_client.py` y `services/bot/app/gemini_client.py`):
- `temperature`: Creatividad (0.0-2.0, default: 0.2)
- `max_tokens`/`maxOutputTokens`: Longitud mÃ¡xima de respuesta (default: 800)
- `model`: Modelo a usar segÃºn proveedor

### TTL de conversaciÃ³n

En `services/bot/app/memory.py`, el TTL por defecto es 24 horas (86400 segundos).

## Desarrollo Local (sin Docker)

```bash
cd services/bot
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

## Troubleshooting

**Error de conexiÃ³n a Redis**: Verifica que el servicio Redis estÃ© corriendo:
```bash
docker compose ps redis
```

**Bot no responde**: Revisa logs:
```bash
docker compose logs -f bot
```

**n8n no recibe webhooks**: AsegÃºrate de exponer n8n con tÃºnel (ngrok/cloudflare) o servidor pÃºblico para que Meta pueda alcanzarlo.

